{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\LogBert\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda\\envs\\LogBert\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "D:\\Anaconda\\envs\\LogBert\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "from loglizer.models import PCA, IsolationForest, LogClustering, OneClassSVM\n",
    "from loglizer import dataloader, preprocessing\n",
    "from loglizer.utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\logbert-main\\loglizer\\dataloader.py:286: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train = np.array(train).reshape(-1,1)\n",
      "E:\\logbert-main\\loglizer\\dataloader.py:292: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_normal = np.array(test_normal).reshape(-1,1)\n",
      "E:\\logbert-main\\loglizer\\dataloader.py:298: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  abnormal = np.array(abnormal).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train normal size: 165180\n",
      "Train abnormal size: 2667\n",
      "Test normal size: 280863\n",
      "Test abnormal size: 4001\n",
      "====== Transformed train data summary ======\n",
      "Train data shape: 167847-by-45\n",
      "\n",
      "====== Transformed test data summary ======\n",
      "Test data shape: 284864-by-45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ouput_dir = \"../output/no_shuffle_hdfs_325/\"\n",
    "(x_train, y_train), (x_test, y_test) = dataloader.load_data(data_dir=ouput_dir)\n",
    "feature_extractor = preprocessing.FeatureExtractor()\n",
    "x_train = feature_extractor.fit_transform(x_train)\n",
    "x_test = feature_extractor.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Model: PCA ====================\n",
      "theshold 0\n",
      "====== Model summary ======\n",
      "n_components: 1\n",
      "Project matrix shape: 45-by-45\n",
      "SPE threshold: 1\n",
      "\n",
      "Train validation:\n",
      "====== Evaluation summary ======\n",
      "Confusion Matrix: TP: 2667, FP: 163006, TN: 2174, FN: 0\n",
      "Precision: 1.610%, recall: 100.000%, F1-measure: 3.169%\n",
      "\n",
      "Test validation:\n",
      "====== Evaluation summary ======\n",
      "Confusion Matrix: TP: 4001, FP: 280863, TN: 0, FN: 0\n",
      "Precision: 1.405%, recall: 100.000%, F1-measure: 2.770%\n",
      "\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"=\"*20 + \" Model: PCA \" + \"=\"*20)\n",
    "for th in np.arange(1):\n",
    "    print(\"theshold\", th)\n",
    "    model = PCA(n_components=0.8, threshold=1, c_alpha = 1.9600)\n",
    "    model.fit(x_train)\n",
    "    print('Train validation:')\n",
    "    precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "    print('Test validation:')\n",
    "    precision, recall, f1 = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Model: IsolationForest ====================\n",
      "====== Model summary ======\n",
      "Train validation:\n",
      "====== Evaluation summary ======\n",
      "Confusion Matrix: TP: 2474, FP: 13333, TN: 151847, FN: 193\n",
      "Precision: 15.651, recall: 92.763, F1-measure: 26.784\n",
      "\n",
      "Test validation:\n",
      "====== Evaluation summary ======\n",
      "Confusion Matrix: TP: 3707, FP: 7205, TN: 273658, FN: 294\n",
      "Precision: 33.972, recall: 92.652, F1-measure: 49.715\n",
      "\n",
      "CPU times: total: 18.4 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"=\"*20 + \" Model: IsolationForest \" + \"=\"*20)\n",
    "model = IsolationForest(n_estimators=100, max_samples='auto', contamination='auto', random_state=19)\n",
    "model.fit(x_train)\n",
    "print('Train validation:')\n",
    "precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "print('Test validation:')\n",
    "precision, recall, f1 = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Model: one class SVM ====================\n",
      "====== Model summary ======\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"=\"*20 + \" Model: one class SVM \" + \"=\"*20)\n",
    "model = OneClassSVM(kernel='rbf')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train validation:')\n",
    "precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "print('Test validation:')\n",
    "precision, recall, f1 = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Model: one class SVM ====================\n",
      "CPU times: user 2min 50s, sys: 3.56 s, total: 2min 54s\n",
      "Wall time: 2min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=OneClassSVM(),\n",
       "             param_grid={'gamma': [0.001, 0.01, 0.1, 1],\n",
       "                         'kernel': ['rbf', 'poly', 'linear', 'sigmoid'],\n",
       "                         'nu': [0.001, 0.01, 0.1, 1]},\n",
       "             scoring='f1_micro')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# print(\"=\"*20 + \" Model: one class SVM \" + \"=\"*20)\n",
    "\n",
    "# nus = [0.001, 0.01, 0.1, 1]\n",
    "# gammas = [0.001, 0.01, 0.1, 1]\n",
    "# tuned_parameters = {'kernel' : ['rbf','poly','linear','sigmoid'], 'gamma' : gammas, 'nu': nus}\n",
    "\n",
    "# ocsvm = svm.OneClassSVM()\n",
    "# model = GridSearchCV(ocsvm, tuned_parameters, cv=5, scoring=\"f1_micro\")\n",
    "\n",
    "# model.fit(x_train, y_train.astype(int))\n",
    "\n",
    "# # print('Train validation:')\n",
    "# # precision, recall, f1 = model.predict(x_train, y_train.astype(int))\n",
    "# # print('Test validation:')\n",
    "# # precision, recall, f1 = model.predict(x_test, y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train validation:\n",
      "Confusion Matrix: TP: 1543, FP: 5000, TN: 0, FN: 957\n",
      "Precision: 23.582, recall: 61.720, F1-measure: 34.126\n",
      "\n",
      "Test validation:\n",
      "Confusion Matrix: TP: 9114, FP: 553223, TN: 0, FN: 5224\n",
      "Precision: 1.621, recall: 63.565, F1-measure: 3.161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('Train validation:')\n",
    "# y_eval = model.predict(x_train)\n",
    "# precision, recall, f1 = metrics(y_eval, y_train)\n",
    "# print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "    \n",
    "# print('Test validation:')\n",
    "# y_pred = model.predict(x_test)\n",
    "# precision, recall, f1 = metrics(y_pred, y_test)\n",
    "# print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Model: LogClustering ====================\n",
      "====== Model summary ======\n",
      "Starting offline clustering...\n",
      "Processed 1000 instances.\n",
      "Found 4 clusters offline.\n",
      "\n",
      "Starting online clustering...\n",
      "Processed 2000 instances.\n",
      "Processed 4000 instances.\n",
      "Processed 6000 instances.\n",
      "Processed 8000 instances.\n",
      "Processed 10000 instances.\n",
      "Processed 12000 instances.\n",
      "Processed 14000 instances.\n",
      "Processed 16000 instances.\n",
      "Processed 18000 instances.\n",
      "Processed 20000 instances.\n",
      "Processed 22000 instances.\n",
      "Processed 24000 instances.\n",
      "Processed 26000 instances.\n",
      "Processed 28000 instances.\n",
      "Processed 30000 instances.\n",
      "Processed 32000 instances.\n",
      "Processed 34000 instances.\n",
      "Processed 36000 instances.\n",
      "Processed 38000 instances.\n",
      "Processed 40000 instances.\n",
      "Processed 42000 instances.\n",
      "Processed 44000 instances.\n",
      "Processed 46000 instances.\n",
      "Processed 48000 instances.\n",
      "Processed 50000 instances.\n",
      "Processed 52000 instances.\n",
      "Processed 54000 instances.\n",
      "Processed 56000 instances.\n",
      "Processed 58000 instances.\n",
      "Processed 60000 instances.\n",
      "Processed 62000 instances.\n",
      "Processed 64000 instances.\n",
      "Processed 66000 instances.\n",
      "Processed 68000 instances.\n",
      "Processed 70000 instances.\n",
      "Processed 72000 instances.\n",
      "Processed 74000 instances.\n",
      "Processed 76000 instances.\n",
      "Processed 78000 instances.\n",
      "Processed 80000 instances.\n",
      "Processed 82000 instances.\n",
      "Processed 84000 instances.\n",
      "Processed 86000 instances.\n",
      "Processed 88000 instances.\n",
      "Processed 90000 instances.\n",
      "Processed 92000 instances.\n",
      "Processed 94000 instances.\n",
      "Processed 96000 instances.\n",
      "Processed 98000 instances.\n",
      "Processed 100000 instances.\n",
      "Processed 102000 instances.\n",
      "Processed 104000 instances.\n",
      "Processed 106000 instances.\n",
      "Processed 108000 instances.\n",
      "Processed 110000 instances.\n",
      "Processed 112000 instances.\n",
      "Processed 114000 instances.\n",
      "Processed 116000 instances.\n",
      "Processed 118000 instances.\n",
      "Processed 120000 instances.\n",
      "Processed 122000 instances.\n",
      "Processed 124000 instances.\n",
      "Processed 126000 instances.\n",
      "Processed 128000 instances.\n",
      "Processed 130000 instances.\n",
      "Processed 132000 instances.\n",
      "Processed 134000 instances.\n",
      "Processed 136000 instances.\n",
      "Processed 138000 instances.\n",
      "Processed 140000 instances.\n",
      "Processed 142000 instances.\n",
      "Processed 144000 instances.\n",
      "Processed 146000 instances.\n",
      "Processed 148000 instances.\n",
      "Processed 150000 instances.\n",
      "Processed 152000 instances.\n",
      "Processed 154000 instances.\n",
      "Processed 156000 instances.\n",
      "Processed 158000 instances.\n",
      "Processed 160000 instances.\n",
      "Processed 162000 instances.\n",
      "Processed 164000 instances.\n",
      "Processed 165180 instances.\n",
      "Found 5 clusters online.\n",
      "\n",
      "Train validation:\n",
      "====== Evaluation summary ======\n",
      "Confusion Matrix: TP: 958, FP: 0, TN: 165180, FN: 1709\n",
      "Precision: 100.000, recall: 35.920, F1-measure: 52.855\n",
      "\n",
      "Test validation:\n",
      "====== Evaluation summary ======\n",
      "Confusion Matrix: TP: 1432, FP: 1, TN: 280862, FN: 2569\n",
      "Precision: 99.930, recall: 35.791, F1-measure: 52.705\n",
      "\n",
      "CPU times: total: 31.2 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"=\"*20 + \" Model: LogClustering \" + \"=\"*20)\n",
    "max_dist = 0.3  # the threshold to stop the clustering process\n",
    "anomaly_threshold = 0.3  # the threshold for anomaly detection\n",
    "model = LogClustering(max_dist=max_dist, anomaly_threshold=anomaly_threshold)\n",
    "model.fit(x_train[y_train == 0, :])  # Use only normal samples for training\n",
    "print('Train validation:')\n",
    "precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "print('Test validation:')\n",
    "precision, recall, f1 = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
